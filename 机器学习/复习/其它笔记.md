## 最大似然与最小二乘法理解

**最大似然估计**：现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。

**最小二乘**：找到一个（组）估计值，使得实际值与估计值的距离最小。本来用两者差的绝对值汇总并使之最小是最理想的，但绝对值在数学上求最小值比较麻烦，因而替代做法是，找一个（组）估计值，使得实际值与估计值之差的平方加总之后的值最小，称为最小二乘。“二乘”的英文为 least square，其实英文的字面意思是“平方最小”。这时，将这个差的平方的和式对参数求导数，并取一阶导数为零，就是 OLSE。

---


说的通俗一点啊，最大似然估计，就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。

例如：一个麻袋里有白球与黑球，但是我不知道它们之间的比例，那我就有放回的抽取10次，结果我发现抽到了8次黑球2次白球，我要求最有可能的黑白球之间的比例时，就采取最大似然估计法：我假设我抽到黑球的概率为 p，那得出8次黑球2次白球这个结果的概率为：

$$P(\text{黑}=8)=p^8(1-p)^2$$

现在我想要得出 p 是多少啊，很简单，使得 $ P(\text{黑}=8) $ 最大的 p 就是我要求的结果，接下来求导的过程就是求极值的过程啦。

可能你会有疑问，为什么要 ln 一下呢，这是因为 ln 把乘法变成加法了，且不会改变极值的位置（单调性保持一致嘛）这样求导会方便很多~

同样，这样一道题：设总体 X 的概率密度为

已知 $ X_1, X_2, ..., X_n $ 是样本观测值，求 θ 的极大似然估计

这也一样啊，要得到 $ X_1, X_2, ..., X_n $ 这样一组样本观测值的概率是

$$
P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n) = f(x_1, \theta) f(x_2, \theta) ... f(x_n, \theta)
$$

然后我们就求使得这个概率最大的 θ 出来啦，一样是求极值的过程，不再赘述。


## 生成式模型与判别式模型

**监督学习方法的两大类别：**

1.  **判别式模型**
    *   **核心思想：** 直接学习条件概率分布 `P(Y|X)`。
    *   **目标：** 找到不同类别之间的最优决策边界。
    *   **代表算法：**
        *   Logistic 回归
        *   决策树
        *   支持向量机

2.  **生成式模型**
    *   **核心思想：** 通过学习联合概率分布 `P(X, Y)` 来间接获得 `P(Y|X)`。具体来说，是先学习类别先验概率 `P(Y)` 和特征的条件概率 `P(X|Y)`，然后利用贝叶斯定理计算 `P(Y|X)`。
    *   **目标：** 对每个类别的数据分布进行建模。
    *   **代表算法：**
        *   朴素贝叶斯方法

---

**总结：**
判别式模型直接对“给定输入X，输出Y是什么”进行建模；而生成式模型则通过学习和模拟数据本身是如何生成的（即`P(X|Y)`和`P(Y)`），来反推输出的可能性。